"""è®­ç»ƒ+éªŒè¯æ–¹æ³•ï¼ŒåŒ…å«TensorBoard"""
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
import os
from tqdm import tqdm


"""æ¨¡åž‹è®­ç»ƒä¸»æµç¨‹"""
def train_model(
        model,
        train_loader,
        val_loader,
        num_epochs,
        device,
        save_path='../checkpoints/checkpoint.pth',    # æ¨¡åž‹ä¿å­˜è·¯å¾„ï¼ˆ.pth æ–‡ä»¶ï¼‰
        resume=False,    # æ˜¯å¦ä»Žæ–­ç‚¹ï¼ˆä¿å­˜çš„æ¨¡åž‹ï¼‰æ¢å¤è®­ç»ƒï¼ˆboolï¼‰
        log_dir='../logs',    # TensorBoard æ—¥å¿—ä¿å­˜ç›®å½•
        lr=0.001,
):
    # ------------------------------------------
    # 1. æ¨¡åž‹å‡†å¤‡
    # ------------------------------------------
    model = model.to(device)

    # Adamä¼˜åŒ–å™¨è‡ªå¸¦ åŠ¨é‡æ³• + è‡ªé€‚åº”å­¦ä¹ çŽ‡
    optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))
    # äº¤å‰ç†µæŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss()

    # å­¦ä¹ çŽ‡è°ƒåº¦å™¨ï¼šæ¯ 10è½® lrÃ—0.1
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)

    # TensorBoard åˆå§‹åŒ–
    writer = SummaryWriter(log_dir=log_dir)

    # åˆå§‹åŒ–æœ€ä½³éªŒè¯å‡†ç¡®çŽ‡å’Œèµ·å§‹è½®æ¬¡
    best_acc = 0.0
    start_epoch = 0

    # ------------------------------------------
    # 2. æ–­ç‚¹ç»­è®­
    # ------------------------------------------
    if resume and os.path.exists(save_path):
        checkpoint = torch.load(save_path)
        model.load_state_dict(checkpoint['model'])            # åŠ è½½æ¨¡åž‹å‚æ•°
        optimizer.load_state_dict(checkpoint['optimizer'])    # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
        scheduler.load_state_dict(checkpoint['scheduler'])    # åŠ è½½è°ƒåº¦å™¨çŠ¶æ€
        start_epoch = checkpoint['epoch'] + 1                 # ä»Žä¸‹ä¸€è½®å¼€å§‹
        best_acc = checkpoint['best_acc']                     # åŠ è½½åŽ†å²æœ€ä½³å‡†ç¡®çŽ‡
        print(f"âœ… Resumed from epoch {start_epoch}, best val acc = {best_acc:.4f}")
    else:
        print("ðŸ“¦ Starting first-time training: loading pretrained ResNet18 weights")
        from torchvision.models import resnet18, ResNet18_Weights
        base_model = resnet18(weights=ResNet18_Weights.DEFAULT)
        model.load_state_dict(base_model.state_dict(), strict=False)

    # ------------------------------------------
    # 3. è®­ç»ƒä¸»å¾ªçŽ¯
    # ------------------------------------------
    for epoch in range(start_epoch, num_epochs):
        model.train()    # è®­ç»ƒæ¨¡å¼
        running_loss = 0.0
        correct = 0    # å½“å‰æ‰¹æ¬¡é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬æ•°
        total = 0      # å½“å‰æ‰¹æ¬¡çš„æ€»æ ·æœ¬æ•°

        # ä½¿ç”¨tqdmæ˜¾ç¤ºè®­ç»ƒè¿›åº¦æ¡
        loop = tqdm(train_loader, desc=f'[Epoch {epoch+1}/{num_epochs}]')

        for i, (images, labels) in enumerate(loop):
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()                # æ¸…ç©ºæ¢¯åº¦
            outputs = model(images)              # å‰å‘ä¼ æ’­
            loss = criterion(outputs, labels)    # è®¡ç®—æŸå¤±
            loss.backward()                      # åå‘ä¼ æ’­
            optimizer.step()                     # æ›´æ–°å‚æ•°

            # ç»Ÿè®¡è®­ç»ƒä¿¡æ¯
            _, preds = torch.max(outputs.data, 1)    # èŽ·å–é¢„æµ‹ç»“æžœ
            running_loss += loss.item()              # ç´¯åŠ æŸå¤±
            correct += (preds==labels).sum().item()    # ç´¯åŠ æ­£ç¡®é¢„æµ‹æ•°
            total += labels.size(0)                  # ç´¯åŠ æ€»æ ·æœ¬æ•°

            # åœ¨è¿›åº¦æ¡ä¸­æ˜¾ç¤ºå½“å‰æŸå¤±å’Œå‡†ç¡®çŽ‡
            loop.set_postfix(loss=loss.item(), acc=correct/total)

        train_loss = running_loss / len(train_loader)    # å¹³å‡è®­ç»ƒæŸå¤±
        train_acc = correct / total                      # å¹³å‡è®­ç»ƒå‡†ç¡®çŽ‡

        # ----------------------------------
        # 4. éªŒè¯é˜¶æ®µï¼šä½¿ç”¨éªŒè¯å‡½æ•°
        # ----------------------------------
        val_loss, val_acc, _, _ = validate_model(model, val_loader, criterion, device)

        # ----------------------------------
        # 5. TensorBoardè®°å½•
        # ----------------------------------
        # è®°å½•æ ‡é‡ï¼ˆæŸå¤±ã€å‡†ç¡®çŽ‡ï¼‰
        writer.add_scalar('Loss/train', train_loss, epoch)
        writer.add_scalar('Loss/val', val_loss, epoch)
        writer.add_scalar('Acc/train', train_acc, epoch)
        writer.add_scalar('Acc/val', val_acc, epoch)
        writer.add_scalars('Compare', {
            'train_acc': train_acc,
            'val_acc': val_acc
        }, epoch)
        # è®°å½•æ¨¡åž‹å‚æ•°ç›´æ–¹å›¾
        writer.add_histogram('fc_weights', model.classifier[-1].weight, epoch)
        # åœ¨ç¬¬ä¸€æ¬¡è®°å½•æ¨¡åž‹ç»“æž„
        if epoch == 0:
            writer.add_graph(model, images)

        # -----------------------------------
        # 6. ä¿å­˜æ¨¡åž‹
        # -----------------------------------
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save({
                'epoch': epoch,
                'model': model.state_dict(),            # æ¨¡åž‹å‚æ•°
                'optimizer': optimizer.state_dict(),    # ä¼˜åŒ–å™¨çŠ¶æ€
                'scheduler': scheduler.state_dict(),    # è°ƒåº¦å™¨çŠ¶æ€
                'best_acc': best_acc                    # æœ€ä½³å‡†ç¡®çŽ‡
            }, save_path)
            print(f"ðŸŒŸSaved new best model at epoch {epoch+1} (acc={best_acc:.4f})")

        # æ›´æ–°å­¦ä¹ çŽ‡
        scheduler.step()

    writer.close()


"""æ¨¡åž‹éªŒè¯é›†æ–¹æ³•"""
def validate_model(model, val_loader, criterion, device):
    model.eval()        # è¯„ä¼°æ¨¡å¼
    total = 0           # æ€»æ ·æœ¬æ•°
    correct = 0         # æ­£ç¡®é¢„æµ‹çš„æ ·æœ¬æ•°
    loss_total = 0.0    # æ€»æŸå¤±

    # ç”¨äºŽç”ŸæˆæŠ¥è¡¨
    y_true_all = []    # è®°å½•æ‰€æœ‰çœŸå®žæ ‡ç­¾
    y_pred_all = []    # è®°å½•æ‰€æœ‰é¢„æµ‹æ ‡ç­¾

    with torch.no_grad():    # å…³é—­æ¢¯åº¦è®¡ç®—
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)    # è®¡ç®—æŸå¤±
            loss_total += loss.item()

            _, preds = torch.max(outputs, 1)    # èŽ·å–é¢„æµ‹ç»“æžœ
            correct += (preds == labels).sum().item()
            total += labels.size(0)

            # ðŸ¬ æ³¨æ„éœ€è¦å°†æ ‡ç­¾ä»ŽGPUç§»åŠ¨åˆ°CPUï¼Œå†è½¬æ¢ä¸ºnumpyæ•°ç»„
            y_true_all.extend(labels.cpu().numpy())    # æ·»åŠ çœŸå®žæ ‡ç­¾
            y_pred_all.extend(preds.cpu().numpy())

    avg_loss = loss_total / len(val_loader)    # éªŒè¯é›†å¹³å‡æŸå¤±
    avg_acc = correct / total                  # éªŒè¯é›†å¹³å‡å‡†ç¡®çŽ‡

    print(f"Validation -> Loss:{avg_loss:.4f} | Acc:{avg_acc:.4f}")
    return avg_loss, avg_acc, y_true_all, y_pred_all